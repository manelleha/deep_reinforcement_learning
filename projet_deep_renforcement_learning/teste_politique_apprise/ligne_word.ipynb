{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1858cda7",
   "metadata": {},
   "source": [
    "### Model entrainer SARSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ef273a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe1a4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chargement réussi depuis : C:\\Users\\elbar\\Downloads\\deep_reinforcement_learning\\projet_deep_renforcement_learning\\sauvgarde\\sarsa_ligne_world\n",
      "\n",
      "🎬 Épisode 1\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "🎬 Épisode 2\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "🎬 Épisode 3\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "🎬 Épisode 4\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "🎬 Épisode 5\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "📈 Gain moyen sur 5 épisodes : 1.00\n",
      "{2: 1, 1: 1, 0: 0, 3: 1, 4: 0}\n",
      "{2: 0.18502631730000002, 1: 0.0, 0: 0.0, 3: 0.5695327900000001, 4: 0.0}\n"
     ]
    }
   ],
   "source": [
    "from algo_sauvgarde import charger_resultats_depuis_chemin, tester_policy_en_console\n",
    "from envs.line_world import reinitialiser, faire_un_pas\n",
    "\n",
    "chemin = r\"C:\\Users\\elbar\\Downloads\\deep_reinforcement_learning\\projet_deep_renforcement_learning\\sauvgarde\\sarsa_ligne_world\"\n",
    "Q, policy, V = charger_resultats_depuis_chemin(chemin)\n",
    "\n",
    "if policy:\n",
    "    tester_policy_en_console(policy, reinitialiser, faire_un_pas, episodes=5)\n",
    "    \n",
    "print(policy)\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140a2631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎮 Déroulement automatique de la politique :\n",
      "\n",
      "🧠 État : 2 → 🎮 Action choisie : 1\n",
      "→ 🌟 Récompense : 0.0 | 🚩 Terminé : False\n",
      "\n",
      "🧠 État : 3 → 🎮 Action choisie : 1\n",
      "→ 🌟 Récompense : 1.0 | 🚩 Terminé : True\n",
      "\n",
      "✅ Fin du déroulement.\n"
     ]
    }
   ],
   "source": [
    "from algo_sauvgarde import jouer_avec_politique,jouer_manuellement\n",
    "# Pour tester automatiquement :\n",
    "jouer_avec_politique(policy, reinitialiser, faire_un_pas, delay=1.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d28bba33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🕹️ Mode manuel activé : choisis toi-même les actions !\n",
      "\n",
      "🧠 État actuel : 2\n",
      "Actions possibles :\n",
      " 0. 0\n",
      " 1. 1\n",
      "→ 🎮 Action : 0 | 🌟 Récompense : 0.0 | 🚩 Terminé : False\n",
      "\n",
      "🧠 État actuel : 1\n",
      "Actions possibles :\n",
      " 0. 0\n",
      " 1. 1\n",
      "→ 🎮 Action : 0 | 🌟 Récompense : -1.0 | 🚩 Terminé : True\n",
      "\n",
      "✅ Fin du test manuel.\n"
     ]
    }
   ],
   "source": [
    "def obtenir_actions(etat):\n",
    "    \"\"\"Retourne la liste des actions possibles pour un état donné (ex: [0, 1] pour ←, →)\"\"\"\n",
    "    return [0, 1]  # gauche et droite dans line_world\n",
    "\n",
    "# Pour tester manuellement :\n",
    "jouer_manuellement(reinitialiser, faire_un_pas, obtenir_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca28f8e4",
   "metadata": {},
   "source": [
    "### Model entrainer Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b24921ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chargement réussi depuis : C:\\Users\\elbar\\Downloads\\deep_reinforcement_learning\\projet_deep_renforcement_learning\\sauvgarde\\q-learning_ligne_world\n",
      "\n",
      "🎬 Épisode 1\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "🎬 Épisode 2\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "🎬 Épisode 3\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "🎬 Épisode 4\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "🎬 Épisode 5\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "📈 Gain moyen sur 5 épisodes : 1.00\n",
      "{2: 1, 1: 1, 0: 0, 3: 1, 4: 0}\n",
      "{2: 0.7982977620220779, 1: 0.0627838576737683, 0: 0.0, 3: 0.9528987130275376, 4: 0.0}\n"
     ]
    }
   ],
   "source": [
    "from algo_sauvgarde import charger_resultats_depuis_chemin, tester_policy_en_console\n",
    "from envs.line_world import reinitialiser, faire_un_pas\n",
    "\n",
    "chemin = r\"C:\\Users\\elbar\\Downloads\\deep_reinforcement_learning\\projet_deep_renforcement_learning\\sauvgarde\\q-learning_ligne_world\"\n",
    "Q, policy, V = charger_resultats_depuis_chemin(chemin)\n",
    "\n",
    "if policy:\n",
    "    tester_policy_en_console(policy, reinitialiser, faire_un_pas, episodes=5)\n",
    "    \n",
    "print(policy)\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a45b15",
   "metadata": {},
   "source": [
    "- tester automatiquement de la politique apprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "508de3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎮 Déroulement automatique de la politique :\n",
      "\n",
      "🧠 État : 2 → 🎮 Action choisie : 1\n",
      "→ 🌟 Récompense : 0.0 | 🚩 Terminé : False\n",
      "\n",
      "🧠 État : 3 → 🎮 Action choisie : 1\n",
      "→ 🌟 Récompense : 1.0 | 🚩 Terminé : True\n",
      "\n",
      "✅ Fin du déroulement.\n"
     ]
    }
   ],
   "source": [
    "from algo_sauvgarde import jouer_avec_politique,jouer_manuellement\n",
    "# Pour tester automatiquement :\n",
    "jouer_avec_politique(policy, reinitialiser, faire_un_pas, delay=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73934ae2",
   "metadata": {},
   "source": [
    "- teste mannuelle de la politique apprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "586c5829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🕹️ Mode manuel activé : choisis toi-même les actions !\n",
      "\n",
      "🧠 État actuel : 2\n",
      "Actions possibles :\n",
      " 0. 0\n",
      " 1. 1\n",
      "→ 🎮 Action : 0 | 🌟 Récompense : 0.0 | 🚩 Terminé : False\n",
      "\n",
      "🧠 État actuel : 1\n",
      "Actions possibles :\n",
      " 0. 0\n",
      " 1. 1\n",
      "→ 🎮 Action : 0 | 🌟 Récompense : -1.0 | 🚩 Terminé : True\n",
      "\n",
      "✅ Fin du test manuel.\n"
     ]
    }
   ],
   "source": [
    "def obtenir_actions(etat):\n",
    "    \"\"\"Retourne la liste des actions possibles pour un état donné (ex: [0, 1] pour ←, →)\"\"\"\n",
    "    return [0, 1]  # gauche et droite dans line_world\n",
    "\n",
    "# Pour tester manuellement :\n",
    "jouer_manuellement(reinitialiser, faire_un_pas, obtenir_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319f25d",
   "metadata": {},
   "source": [
    "### Model entrainer dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45691f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chargement réussi depuis : C:\\Users\\elbar\\Downloads\\deep_reinforcement_learning\\projet_deep_renforcement_learning\\sauvgarde\\q-dyna_ligne_world\n",
      "\n",
      "🎬 Épisode 1\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "🎬 Épisode 2\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "🎬 Épisode 3\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "🎬 Épisode 4\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "🎬 Épisode 5\n",
      "🧠 État: 2, 🎮 Action: 1\n",
      "🧠 État: 3, 🎮 Action: 1\n",
      "🏁 Terminé à l'état 4 avec gain 1.0\n",
      "\n",
      "📈 Gain moyen sur 5 épisodes : 1.00\n",
      "{2: 1, 1: 1, 0: 0, 3: 1, 4: 0}\n",
      "{2: 0.8999999999999979, 1: 0.8099999999999972, 0: 0.0, 3: 0.9999999999999989, 4: 0.0}\n"
     ]
    }
   ],
   "source": [
    "from algo_sauvgarde import charger_resultats_depuis_chemin, tester_policy_en_console\n",
    "from envs.line_world import reinitialiser, faire_un_pas\n",
    "\n",
    "chemin = r\"C:\\Users\\elbar\\Downloads\\deep_reinforcement_learning\\projet_deep_renforcement_learning\\sauvgarde\\q-dyna_ligne_world\"\n",
    "Q, policy, V = charger_resultats_depuis_chemin(chemin)\n",
    "\n",
    "if policy:\n",
    "    tester_policy_en_console(policy, reinitialiser, faire_un_pas, episodes=5)\n",
    "    \n",
    "print(policy)\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a686c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎮 Déroulement automatique de la politique :\n",
      "\n",
      "🧠 État : 2 → 🎮 Action choisie : 1\n",
      "→ 🌟 Récompense : 0.0 | 🚩 Terminé : False\n",
      "\n",
      "🧠 État : 3 → 🎮 Action choisie : 1\n",
      "→ 🌟 Récompense : 1.0 | 🚩 Terminé : True\n",
      "\n",
      "✅ Fin du déroulement.\n"
     ]
    }
   ],
   "source": [
    "from algo_sauvgarde import jouer_avec_politique,jouer_manuellement\n",
    "# Pour tester automatiquement :\n",
    "jouer_avec_politique(policy, reinitialiser, faire_un_pas, delay=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e461f9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🕹️ Mode manuel activé : choisis toi-même les actions !\n",
      "\n",
      "🧠 État actuel : 2\n",
      "Actions possibles :\n",
      " 0. 0\n",
      " 1. 1\n",
      "→ 🎮 Action : 1 | 🌟 Récompense : 0.0 | 🚩 Terminé : False\n",
      "\n",
      "🧠 État actuel : 3\n",
      "Actions possibles :\n",
      " 0. 0\n",
      " 1. 1\n",
      "→ 🎮 Action : 1 | 🌟 Récompense : 1.0 | 🚩 Terminé : True\n",
      "\n",
      "✅ Fin du test manuel.\n"
     ]
    }
   ],
   "source": [
    "def obtenir_actions(etat):\n",
    "    \"\"\"Retourne la liste des actions possibles pour un état donné (ex: [0, 1] pour ←, →)\"\"\"\n",
    "    return [0, 1]  # gauche et droite dans line_world\n",
    "\n",
    "# Pour tester manuellement :\n",
    "jouer_manuellement(reinitialiser, faire_un_pas, obtenir_actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
