{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1858cda7",
   "metadata": {},
   "source": [
    "### Model entrainer SARSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ef273a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe1a4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chargement rÃ©ussi depuis : C:\\Users\\elbar\\Downloads\\deep_reinforcement_learning\\projet_deep_renforcement_learning\\sauvgarde\\sarsa_ligne_world\n",
      "\n",
      "ğŸ¬ Ã‰pisode 1\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ¬ Ã‰pisode 2\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ¬ Ã‰pisode 3\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ¬ Ã‰pisode 4\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ¬ Ã‰pisode 5\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ“ˆ Gain moyen sur 5 Ã©pisodes : 1.00\n",
      "{2: 1, 1: 1, 0: 0, 3: 1, 4: 0}\n",
      "{2: 0.18502631730000002, 1: 0.0, 0: 0.0, 3: 0.5695327900000001, 4: 0.0}\n"
     ]
    }
   ],
   "source": [
    "from algo_sauvgarde import charger_resultats_depuis_chemin, tester_policy_en_console\n",
    "from envs.line_world import reinitialiser, faire_un_pas\n",
    "\n",
    "chemin = r\"C:\\Users\\elbar\\Downloads\\deep_reinforcement_learning\\projet_deep_renforcement_learning\\sauvgarde\\sarsa_ligne_world\"\n",
    "Q, policy, V = charger_resultats_depuis_chemin(chemin)\n",
    "\n",
    "if policy:\n",
    "    tester_policy_en_console(policy, reinitialiser, faire_un_pas, episodes=5)\n",
    "    \n",
    "print(policy)\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140a2631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ® DÃ©roulement automatique de la politique :\n",
      "\n",
      "ğŸ§  Ã‰tat : 2 â†’ ğŸ® Action choisie : 1\n",
      "â†’ ğŸŒŸ RÃ©compense : 0.0 | ğŸš© TerminÃ© : False\n",
      "\n",
      "ğŸ§  Ã‰tat : 3 â†’ ğŸ® Action choisie : 1\n",
      "â†’ ğŸŒŸ RÃ©compense : 1.0 | ğŸš© TerminÃ© : True\n",
      "\n",
      "âœ… Fin du dÃ©roulement.\n"
     ]
    }
   ],
   "source": [
    "from algo_sauvgarde import jouer_avec_politique,jouer_manuellement\n",
    "# Pour tester automatiquement :\n",
    "jouer_avec_politique(policy, reinitialiser, faire_un_pas, delay=1.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d28bba33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ•¹ï¸ Mode manuel activÃ© : choisis toi-mÃªme les actions !\n",
      "\n",
      "ğŸ§  Ã‰tat actuel : 2\n",
      "Actions possibles :\n",
      " 0. 0\n",
      " 1. 1\n",
      "â†’ ğŸ® Action : 0 | ğŸŒŸ RÃ©compense : 0.0 | ğŸš© TerminÃ© : False\n",
      "\n",
      "ğŸ§  Ã‰tat actuel : 1\n",
      "Actions possibles :\n",
      " 0. 0\n",
      " 1. 1\n",
      "â†’ ğŸ® Action : 0 | ğŸŒŸ RÃ©compense : -1.0 | ğŸš© TerminÃ© : True\n",
      "\n",
      "âœ… Fin du test manuel.\n"
     ]
    }
   ],
   "source": [
    "def obtenir_actions(etat):\n",
    "    \"\"\"Retourne la liste des actions possibles pour un Ã©tat donnÃ© (ex: [0, 1] pour â†, â†’)\"\"\"\n",
    "    return [0, 1]  # gauche et droite dans line_world\n",
    "\n",
    "# Pour tester manuellement :\n",
    "jouer_manuellement(reinitialiser, faire_un_pas, obtenir_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca28f8e4",
   "metadata": {},
   "source": [
    "### Model entrainer Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b24921ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chargement rÃ©ussi depuis : C:\\Users\\elbar\\Downloads\\deep_reinforcement_learning\\projet_deep_renforcement_learning\\sauvgarde\\q-learning_ligne_world\n",
      "\n",
      "ğŸ¬ Ã‰pisode 1\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ¬ Ã‰pisode 2\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ¬ Ã‰pisode 3\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ¬ Ã‰pisode 4\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ¬ Ã‰pisode 5\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ“ˆ Gain moyen sur 5 Ã©pisodes : 1.00\n",
      "{2: 1, 1: 1, 0: 0, 3: 1, 4: 0}\n",
      "{2: 0.7982977620220779, 1: 0.0627838576737683, 0: 0.0, 3: 0.9528987130275376, 4: 0.0}\n"
     ]
    }
   ],
   "source": [
    "from algo_sauvgarde import charger_resultats_depuis_chemin, tester_policy_en_console\n",
    "from envs.line_world import reinitialiser, faire_un_pas\n",
    "\n",
    "chemin = r\"C:\\Users\\elbar\\Downloads\\deep_reinforcement_learning\\projet_deep_renforcement_learning\\sauvgarde\\q-learning_ligne_world\"\n",
    "Q, policy, V = charger_resultats_depuis_chemin(chemin)\n",
    "\n",
    "if policy:\n",
    "    tester_policy_en_console(policy, reinitialiser, faire_un_pas, episodes=5)\n",
    "    \n",
    "print(policy)\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a45b15",
   "metadata": {},
   "source": [
    "- tester automatiquement de la politique apprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "508de3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ® DÃ©roulement automatique de la politique :\n",
      "\n",
      "ğŸ§  Ã‰tat : 2 â†’ ğŸ® Action choisie : 1\n",
      "â†’ ğŸŒŸ RÃ©compense : 0.0 | ğŸš© TerminÃ© : False\n",
      "\n",
      "ğŸ§  Ã‰tat : 3 â†’ ğŸ® Action choisie : 1\n",
      "â†’ ğŸŒŸ RÃ©compense : 1.0 | ğŸš© TerminÃ© : True\n",
      "\n",
      "âœ… Fin du dÃ©roulement.\n"
     ]
    }
   ],
   "source": [
    "from algo_sauvgarde import jouer_avec_politique,jouer_manuellement\n",
    "# Pour tester automatiquement :\n",
    "jouer_avec_politique(policy, reinitialiser, faire_un_pas, delay=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73934ae2",
   "metadata": {},
   "source": [
    "- teste mannuelle de la politique apprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "586c5829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ•¹ï¸ Mode manuel activÃ© : choisis toi-mÃªme les actions !\n",
      "\n",
      "ğŸ§  Ã‰tat actuel : 2\n",
      "Actions possibles :\n",
      " 0. 0\n",
      " 1. 1\n",
      "â†’ ğŸ® Action : 0 | ğŸŒŸ RÃ©compense : 0.0 | ğŸš© TerminÃ© : False\n",
      "\n",
      "ğŸ§  Ã‰tat actuel : 1\n",
      "Actions possibles :\n",
      " 0. 0\n",
      " 1. 1\n",
      "â†’ ğŸ® Action : 0 | ğŸŒŸ RÃ©compense : -1.0 | ğŸš© TerminÃ© : True\n",
      "\n",
      "âœ… Fin du test manuel.\n"
     ]
    }
   ],
   "source": [
    "def obtenir_actions(etat):\n",
    "    \"\"\"Retourne la liste des actions possibles pour un Ã©tat donnÃ© (ex: [0, 1] pour â†, â†’)\"\"\"\n",
    "    return [0, 1]  # gauche et droite dans line_world\n",
    "\n",
    "# Pour tester manuellement :\n",
    "jouer_manuellement(reinitialiser, faire_un_pas, obtenir_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319f25d",
   "metadata": {},
   "source": [
    "### Model entrainer dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45691f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chargement rÃ©ussi depuis : C:\\Users\\elbar\\Downloads\\deep_reinforcement_learning\\projet_deep_renforcement_learning\\sauvgarde\\q-dyna_ligne_world\n",
      "\n",
      "ğŸ¬ Ã‰pisode 1\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ¬ Ã‰pisode 2\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ¬ Ã‰pisode 3\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ¬ Ã‰pisode 4\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ¬ Ã‰pisode 5\n",
      "ğŸ§  Ã‰tat: 2, ğŸ® Action: 1\n",
      "ğŸ§  Ã‰tat: 3, ğŸ® Action: 1\n",
      "ğŸ TerminÃ© Ã  l'Ã©tat 4 avec gain 1.0\n",
      "\n",
      "ğŸ“ˆ Gain moyen sur 5 Ã©pisodes : 1.00\n",
      "{2: 1, 1: 1, 0: 0, 3: 1, 4: 0}\n",
      "{2: 0.8999999999999979, 1: 0.8099999999999972, 0: 0.0, 3: 0.9999999999999989, 4: 0.0}\n"
     ]
    }
   ],
   "source": [
    "from algo_sauvgarde import charger_resultats_depuis_chemin, tester_policy_en_console\n",
    "from envs.line_world import reinitialiser, faire_un_pas\n",
    "\n",
    "chemin = r\"C:\\Users\\elbar\\Downloads\\deep_reinforcement_learning\\projet_deep_renforcement_learning\\sauvgarde\\q-dyna_ligne_world\"\n",
    "Q, policy, V = charger_resultats_depuis_chemin(chemin)\n",
    "\n",
    "if policy:\n",
    "    tester_policy_en_console(policy, reinitialiser, faire_un_pas, episodes=5)\n",
    "    \n",
    "print(policy)\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a686c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ® DÃ©roulement automatique de la politique :\n",
      "\n",
      "ğŸ§  Ã‰tat : 2 â†’ ğŸ® Action choisie : 1\n",
      "â†’ ğŸŒŸ RÃ©compense : 0.0 | ğŸš© TerminÃ© : False\n",
      "\n",
      "ğŸ§  Ã‰tat : 3 â†’ ğŸ® Action choisie : 1\n",
      "â†’ ğŸŒŸ RÃ©compense : 1.0 | ğŸš© TerminÃ© : True\n",
      "\n",
      "âœ… Fin du dÃ©roulement.\n"
     ]
    }
   ],
   "source": [
    "from algo_sauvgarde import jouer_avec_politique,jouer_manuellement\n",
    "# Pour tester automatiquement :\n",
    "jouer_avec_politique(policy, reinitialiser, faire_un_pas, delay=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e461f9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ•¹ï¸ Mode manuel activÃ© : choisis toi-mÃªme les actions !\n",
      "\n",
      "ğŸ§  Ã‰tat actuel : 2\n",
      "Actions possibles :\n",
      " 0. 0\n",
      " 1. 1\n",
      "â†’ ğŸ® Action : 1 | ğŸŒŸ RÃ©compense : 0.0 | ğŸš© TerminÃ© : False\n",
      "\n",
      "ğŸ§  Ã‰tat actuel : 3\n",
      "Actions possibles :\n",
      " 0. 0\n",
      " 1. 1\n",
      "â†’ ğŸ® Action : 1 | ğŸŒŸ RÃ©compense : 1.0 | ğŸš© TerminÃ© : True\n",
      "\n",
      "âœ… Fin du test manuel.\n"
     ]
    }
   ],
   "source": [
    "def obtenir_actions(etat):\n",
    "    \"\"\"Retourne la liste des actions possibles pour un Ã©tat donnÃ© (ex: [0, 1] pour â†, â†’)\"\"\"\n",
    "    return [0, 1]  # gauche et droite dans line_world\n",
    "\n",
    "# Pour tester manuellement :\n",
    "jouer_manuellement(reinitialiser, faire_un_pas, obtenir_actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
