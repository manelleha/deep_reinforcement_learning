{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f13f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab24c39",
   "metadata": {},
   "source": [
    "### Temporal_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6a5e9",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2b47f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Ã‰pisode 1/100\n",
      "âœ… Exploitation : meilleure action selon Q â†’ 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "faire_un_pas() missing 1 required positional argument: 'action_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m etats_suivis \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrock\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaper\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscissors\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Appel Ã  lâ€™algorithme (exemple avec SARSA)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m Q, df_q \u001b[38;5;241m=\u001b[39m sarsa_q_learning(\n\u001b[0;32m     10\u001b[0m     reinitialiser\u001b[38;5;241m=\u001b[39mreinitialiser,\n\u001b[0;32m     11\u001b[0m     faire_un_pas\u001b[38;5;241m=\u001b[39mfaire_un_pas,\n\u001b[0;32m     12\u001b[0m     obtenir_actions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m e: [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m],  \u001b[38;5;66;03m# 0: rock, 1: paper, 2: scissors\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     14\u001b[0m     alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m     15\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m,\n\u001b[0;32m     16\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     17\u001b[0m     etats_suivis\u001b[38;5;241m=\u001b[39metats_suivis,\n\u001b[0;32m     18\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msarsa\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# ou \"q_learning\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\elbar\\Downloads\\projet_deep_renforcement_learning\\notebooks\\..\\algos\\temporal_difference_learning.py:67\u001b[0m, in \u001b[0;36msarsa_q_learning\u001b[1;34m(reinitialiser, faire_un_pas, obtenir_actions, episodes, alpha, gamma, epsilon, etats_suivis, mode, verbose)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_learning\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     65\u001b[0m     action \u001b[38;5;241m=\u001b[39m politique_epsilon_greedy(Q, etat, obtenir_actions(etat), epsilon, verbose)\n\u001b[1;32m---> 67\u001b[0m etat_suiv, recompense, termine \u001b[38;5;241m=\u001b[39m faire_un_pas(action)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m termine:\n\u001b[0;32m     70\u001b[0m     Q[(etat, action)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m (recompense \u001b[38;5;241m-\u001b[39m Q[(etat, action)])\n",
      "\u001b[1;31mTypeError\u001b[0m: faire_un_pas() missing 1 required positional argument: 'action_index'"
     ]
    }
   ],
   "source": [
    "from algos.temporal_difference_learning import sarsa_q_learning\n",
    "from envs.rock_paper_scissors import reinitialiser, faire_un_pas, actions \n",
    "from algos.politique import politique_epsilon_greedy\n",
    "\n",
    "# DÃ©finir les Ã©tats Ã  suivre pour traquer l'Ã©volution des Q-valeurs\n",
    "etats_suivis = [(0, None), (1, \"rock\"), (1, \"paper\"), (1, \"scissors\")]\n",
    "\n",
    "# Appel Ã  lâ€™algorithme (exemple avec SARSA)\n",
    "Q, df_q = sarsa_q_learning(\n",
    "    reinitialiser=reinitialiser,\n",
    "    faire_un_pas=faire_un_pas,\n",
    "    obtenir_actions=lambda e: [0, 1, 2],  # 0: rock, 1: paper, 2: scissors\n",
    "    episodes=100,\n",
    "    alpha=0.5,\n",
    "    gamma=0.99,\n",
    "    epsilon=0.1,\n",
    "    etats_suivis=etats_suivis,\n",
    "    mode=\"sarsa\",  # ou \"q_learning\"\n",
    "    verbose=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
