import random
from collections import defaultdict
import pandas as pd
import numpy as np
from tqdm import trange, tqdm
import matplotlib.pyplot as plt


def politique_epsilon_greedy(Q, √©tat, actions, epsilon, verbose=False):
    """Politique Œµ-greedy pour la s√©lection d'actions"""
    if random.uniform(0, 1) < epsilon:
        action = random.choice(actions)
        if verbose:
            tqdm.write(f"üîÄ Exploration : action al√©atoire ‚Üí {action}")
        return action
    else:
        valeurs_q = [Q[(√©tat, a)] for a in actions]
        index_max = np.argmax(valeurs_q)
        action = actions[index_max]
        if verbose:
            tqdm.write(f"‚úÖ Exploitation : meilleure action selon Q ‚Üí {action}")
        return action

def dyna_q(
    reinitialiser,
    faire_un_pas,
    obtenir_actions,
    episodes=10000,
    alpha=0.1,
    gamma=0.99,
    epsilon=0.1,
    planning_steps=1000,
    etats_suivis=None,
    verbose=False
):
    Q = defaultdict(float)    # Q(s, a)
    model = dict()            # Model(s, a) = (R, S')
    suivi_q = []
    loss_episode = []

    for episode in trange(episodes, desc="Entra√Ænement Dyna-Q", leave=True):
        S = reinitialiser()
        episode_losses = []
        pas = 0

        while S is not None:
            actions = obtenir_actions(S)
            if not actions:
                break

            A = politique_epsilon_greedy(Q, S, actions, epsilon, verbose)

            S_prime, R, termin√© = faire_un_pas(A)

            # Mise √† jour Q-learning
            if termin√© or S_prime is None:
                td_target = R
            else:
                actions_suivantes = obtenir_actions(S_prime)
                td_target = R + gamma * max(Q[(S_prime, a)] for a in actions_suivantes) if actions_suivantes else R

            td_error = td_target - Q[(S, A)]
            Q[(S, A)] += alpha * td_error

            loss = 0.5 * td_error**2
            episode_losses.append(loss)

            # Mise √† jour du mod√®le
            model[(S, A)] = (R, S_prime)

            if verbose:
                tqdm.write(f"\nüéØ √âpisode {episode + 1} - Pas {pas}")
                tqdm.write(f"üß† √âtat     : {S}")
                tqdm.write(f"üéÆ Action   : {A}")
                tqdm.write(f"üí∞ R√©compense : {R}")
                tqdm.write(f"üìä Q({S}, {A}) = {Q[(S, A)]:.4f}")
                tqdm.write(f"üìâ TD Error  : {td_error:.4f}")
                tqdm.write(f"üìâ Loss      : {loss:.4f}")
                tqdm.write("üì¶ Mod√®le actuel :")
                for (s_mod, a_mod), (r_mod, s_p_mod) in model.items():
                    tqdm.write(f"  ({s_mod}, {a_mod}) ‚Üí (R={r_mod}, S'={s_p_mod})")
                tqdm.write("-" * 60)

            # Phase de planification
            for _ in range(planning_steps):
                S_sim, A_sim = random.choice(list(model.keys()))
                R_sim, S_prime_sim = model[(S_sim, A_sim)]
                actions_sim = obtenir_actions(S_prime_sim) if S_prime_sim is not None else []

                target_sim = R_sim + gamma * max(Q[(S_prime_sim, a)] for a in actions_sim) if actions_sim else R_sim
                Q[(S_sim, A_sim)] += alpha * (target_sim - Q[(S_sim, A_sim)])

            if termin√©:
                break

            S = S_prime
            pas += 1

        moyenne_loss = np.mean(episode_losses) if episode_losses else 0
        loss_episode.append(moyenne_loss)

        if etats_suivis:
            snapshot = {
                f"{e}-{a}": Q[(e, a)]
                for e in etats_suivis
                for a in obtenir_actions(e)
                if obtenir_actions(e)
            }
            snapshot["√©pisode"] = episode
            snapshot["loss_moyenne"] = moyenne_loss
            suivi_q.append(snapshot)

    df_q = pd.DataFrame(suivi_q)
    return Q, df_q, loss_episode

def afficher_politique(Q, etats, actions, etats_terminaux):
    print("\nüß† Politique apprise (greedy) :")
    for etat in etats:
        if etat in etats_terminaux:
            print(f"√âtat {etat} (terminal) : ‚õî")
        else:
            meilleure_action = max(actions, key=lambda a: Q[(etat, a)])
            symbole = "‚Üê" if meilleure_action == 0 else "‚Üí"
            print(f"√âtat {etat} : action optimale = {meilleure_action} {symbole} (Q = {Q[(etat, meilleure_action)]:.2f})")




#========================================================================
# √âvaluer la politique fig√©e
#========================================================================
def tester_politique_figee(Q, reinitialiser, faire_un_pas, obtenir_actions, episodes=100, verbose=False):
    total_gain = 0

    for ep in range(episodes):
        etat = reinitialiser()
        gain = 0
        termin√© = False
        pas = 0

        if verbose:
            print(f"\nüß™ √âvaluation - √âpisode {ep+1}")

        while not termin√© and etat is not None:
            actions = obtenir_actions(etat)
            if not actions:
                break

            action = max(actions, key=lambda a: Q[(etat, a)])  # politique fig√©e : greedy
            etat_suiv, recompense, termin√© = faire_un_pas(action)
            gain += recompense

            if verbose:
                print(f"  Pas {pas} : √©tat={etat}, action={action}, r√©compense={recompense}, √©tat_suiv={etat_suiv}")
            
            etat = etat_suiv
            pas += 1

        total_gain += gain
        if verbose:
            print(f"üéØ Gain total √©pisode {ep+1} : {gain}")

    gain_moyen = total_gain / episodes
    print("\nüìà √âvaluation de la strat√©gie fig√©e :")
    print(f"üéØ Gain total sur {episodes} √©pisodes : {total_gain}")
    print(f"üìä Gain moyen par √©pisode           : {gain_moyen:.2f}")

    return gain_moyen
