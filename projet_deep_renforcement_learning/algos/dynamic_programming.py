import numpy as np
import pandas as pd
from tqdm import trange
from collections import defaultdict

def policy_iteration_dp(
    etats,
    actions,
    p,  # Tenseur de probabilitÃ©s [Ã©tat, action, Ã©tat_suivant, rÃ©compense_index]
    recompenses,
    etats_terminaux,
    gamma=0.99,
    theta=0.00001,
    max_iterations=100,
    max_eval_iterations=100,
    verbose=False
):
    
    
    # Initialisation
    V = {s: 0.0 for s in etats}
    
    # Politique initiale : premiÃ¨re action valide pour chaque Ã©tat
    policy = {}
    for s in etats:
        actions_valides = [a for a in actions if np.sum(p[s, a]) > 0]
        policy[s] = actions_valides[0] if actions_valides else actions[0]
    
    historique_loss = []
    historique_V = []
    
    if verbose:
        print(f"ðŸ§  Ã‰tats : {etats}")
        print(f"ðŸŽ® Actions : {actions}")
        print(f"ðŸ Ã‰tats terminaux : {etats_terminaux}")
        print(f"ðŸ”„ DÃ©but de Policy Iteration (Dynamic Programming)...")
    
    for it in trange(max_iterations, desc="Policy Iteration DP"):
        
        # ============== Ã‰VALUATION DE LA POLITIQUE ====================
        if verbose:
            print(f"\nðŸ“ˆ Ã‰valuation de la politique (itÃ©ration {it+1})")
            
        for eval_it in range(max_eval_iterations):
            delta = 0
            V_new = V.copy()
            
            for s in etats:
                if s in etats_terminaux:
                    continue  # Les Ã©tats terminaux gardent V=0
                    
                a = policy[s]
                v_old = V[s]
                
                # Calcul exact avec les probabilitÃ©s de transition
                v_new = 0.0
                for s_next in etats:
                    for r_idx in range(len(recompenses)):
                        prob = p[s, a, s_next, r_idx]
                        if prob > 0:
                            reward = recompenses[r_idx]
                            if s_next in etats_terminaux:
                                v_new += prob * reward  # Pas de continuation si terminal
                            else:
                                v_new += prob * (reward + gamma * V[s_next])
                
                V_new[s] = v_new
                delta = max(delta, abs(v_old - v_new))
                
                if verbose:
                    print(f"ðŸ§  Ã‰tat {s} | ðŸŽ® Action {a} | ðŸ“Š V: {v_old:.4f} â†’ {v_new:.4f}")
            
            V = V_new
            
            if verbose:
                print(f"ðŸ“‰ Delta max = {delta:.6f}")
                
            if delta < theta:
                if verbose:
                    print(f"âœ… Convergence atteinte en {eval_it+1} itÃ©rations d'Ã©valuation")
                break
        
        historique_loss.append(delta)
        historique_V.append(dict(V))
        
        # ============== AMÃ‰LIORATION DE LA POLITIQUE ====================
        if verbose:
            print("ðŸŽ¯ AmÃ©lioration de la politique...")
            
        policy_stable = True
        
        for s in etats:
            if s in etats_terminaux:
                continue  # Pas d'action dans les Ã©tats terminaux
                
            old_action = policy[s]
            
            # Calculer Q-values pour toutes les actions valides
            q_values = {}
            actions_valides = [a for a in actions if np.sum(p[s, a]) > 0]
            
            for a in actions_valides:
                q_val = 0.0
                for s_next in etats:
                    for r_idx in range(len(recompenses)):
                        prob = p[s, a, s_next, r_idx]
                        if prob > 0:
                            reward = recompenses[r_idx]
                            if s_next in etats_terminaux:
                                q_val += prob * reward
                            else:
                                q_val += prob * (reward + gamma * V[s_next])
                
                q_values[a] = q_val
            
            # Choisir la meilleure action
            if q_values:
                best_action = max(q_values, key=q_values.get)
                policy[s] = best_action
                
                if verbose:
                    print(f"ðŸ§  Ã‰tat {s} | ðŸ” {old_action} â†’ ðŸŽ® {best_action}")
                    for act, val in q_values.items():
                        marker = "ðŸ†" if act == best_action else "  "
                        print(f"    {marker} Q({s},{act}) = {val:.4f}")
                
                if best_action != old_action:
                    policy_stable = False
        
        if verbose:
            print(f"âœ… Politique stable : {policy_stable}")
            
        if policy_stable:
            if verbose:
                print(f"ðŸŽ‰ Convergence de la politique en {it+1} itÃ©rations !")
            break
    
    # CrÃ©er DataFrame avec l'historique des valeurs
    df_v = pd.DataFrame(historique_V).fillna(0)
    
    return policy, dict(V), df_v, historique_loss


def value_iteration_dp(
    etats,
    actions,
    p,
    recompenses,
    etats_terminaux,
    gamma=0.99,
    theta=0.00001,
    max_iterations=100,
    verbose=False
):
    # Initialisation des valeurs d'Ã©tat
    V = {s: 0.0 for s in etats}
    historique_loss = []
    historique_V = []

    if verbose:
        print(f"ðŸ§  Ã‰tats : {etats}")
        print(f"ðŸŽ® Actions : {actions}")
        print(f"ðŸ Ã‰tats terminaux : {etats_terminaux}")
        print(f"ðŸ”„ DÃ©but de Value Iteration (Dynamic Programming)...")

    for it in trange(max_iterations, desc="Value Iteration DP"):
        delta = 0
        V_new = V.copy()

        for s in etats:
            if s in etats_terminaux:
                continue  # Les Ã©tats terminaux restent Ã  0

            v_old = V[s]
            q_values = []

            for a in actions:
                q_val = 0.0
                for s_next in etats:
                    for r_idx in range(len(recompenses)):
                        prob = p[s, a, s_next, r_idx]
                        if prob > 0:
                            r = recompenses[r_idx]
                            if s_next in etats_terminaux:
                                q_val += prob * r
                            else:
                                q_val += prob * (r + gamma * V[s_next])
                q_values.append(q_val)

            V_new[s] = max(q_values)
            delta = max(delta, abs(v_old - V_new[s]))

            if verbose:
                print(f"ðŸ§  Ã‰tat {s} | ðŸ“Š V: {v_old:.4f} â†’ {V_new[s]:.4f}")

        V = V_new
        historique_loss.append(delta)
        historique_V.append(dict(V))

        if verbose:
            print(f"ðŸ“‰ Delta max = {delta:.6f}")

        if delta < theta:
            if verbose:
                print(f"âœ… Convergence atteinte Ã  l'itÃ©ration {it+1}")
            break

    # Politique dÃ©terministe
    policy = {}
    for s in etats:
        if s in etats_terminaux:
            policy[s] = None
            continue

        best_a = None
        best_q = -np.inf

        for a in actions:
            q_val = 0.0
            for s_next in etats:
                for r_idx in range(len(recompenses)):
                    prob = p[s, a, s_next, r_idx]
                    if prob > 0:
                        r = recompenses[r_idx]
                        if s_next in etats_terminaux:
                            q_val += prob * r
                        else:
                            q_val += prob * (r + gamma * V[s_next])

            if q_val > best_q:
                best_q = q_val
                best_a = a

        policy[s] = best_a

        if verbose:
            print(f"ðŸŽ® Politique[{s}] = {best_a} avec Q = {best_q:.4f}")

    df_v = pd.DataFrame(historique_V).fillna(0)

    return policy, dict(V), df_v, historique_loss 